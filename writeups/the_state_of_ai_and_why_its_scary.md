---
Title: The state of AI, what essentially it is and why it's scary😱
Tags: #CS #ComputerScience #AI #TechnicalExtrapolation #StatusUpdate
Wrote on: 17th of April, 2023
---

I love freaking computers. Internet, even more so. I vividly remember the time when I got this machine at first. It was glorious. I was a kid and all I wanted to do was to play on it. Once I crunched through all the games available to me(Thanks Mamu) I started hacking the system and started modding the existing applications within it. To the point where I even broke it completely and has to reset the entire system, with freaking Rs.500(a very big thing in 2003/4 in Udhampur). Ah, the sweet days. It(the computer) was dumb and I was in control.

## Things are … well … happening too fast

Not anymore. These machines now work collectively with organisations and have the power to enslave us. Attention-wise. Example being Facebook, Twitter, etc. Remember the stuff that’s happened with Social media in 2010’s and until recently.

And that's just with algorithmic programming and with very dumb and narrowly focused AI systems. Consider Instagram. It’s UX/UI along with the existing backend system is designed for us to stay hooked on it. For hours if we don't put ourselves in check. Entertainers needed so much effort to achieve this. Now it's easy, scalable and accessible to all.

And now AI. I'm still familiarizing myself with ChatGPT(I am used to Co-pilot now). And now tools like GPT-4 and AutoGPT are out in the wild. I'm attempting to contribute to AutoGPT as I'm writing this, for I don’t miss out on upskilling myself into collaboratively working with these AI tools. It's scary yet exciting.

## All mathematical functions

To me all Algorithms, Computer programs and everything running on a computer system is essentially a mathematical function. Infact everything designed by humans and life in itself, including our thought processes are nothing but mathematical functions. They all have specific inputs and specific outputs. The inputs can range across multiple vectors as in ‘money’ being one vector and ‘the news of broken war between China & Taiwan’ being another. Other vectors can be present and in a multitude of formats(For which we’re seeing these DBs now … PineconeDB, etc.). The computation performed is a mathematical function  that can be used to determine whether this event causes any rise/fall in the stock.

Deep learning models and these LLMs in use for generative models are nothing but mathematical aggregations 

> In essence, all these neural networks are essentially just aggregation functions.
> 

That’s what I want to think it to be … until…

## There’s a certain threshold

I worked on one generative model myself once. And the bottle-neck was raw compute power and memory only. I used up 900$ worth of GPU credits, free CUDA GPUs on Collab and my own machines(scavenged from some friends as well) to train Deep-Learning models. I learnt a lot. And with limited number of parameters(don’t exactly remember the count but it was nearly 1-2 million in the version I presented for my Bachelors) they didn’t turn up as well. With only passage of ~30% of the Turing tests we performed. I was so disappointed after a certain point that I stopped learning about AI and moved into learning about hardcore development work, game development and Cybersecurity. I fell more in love with Cybersecurity though.

But these LLMs are very big. In orders of billions. And after reaching a certain threshold they start behaving like real human intelligence. Almost to the point where AI generated content is indistinguishable from human made ones. ChatGPT is a big example. So is DALL-E.

**********************A small story:********************** I draw out to relax and cool down after my day of work/studying. And to grab ideas I visit Pinterest, Instagram and Tumblr. And earlier AI generated artwork was distinguishable. Now it isn’t, unless I look at the associated tags. And personally I’ve stopped visiting those platforms due that very reason(reason being there’s no difference between real and fake). The growth is exponential and has happened only within last three months. It’s f**king overwhelming. Trillion parameter models are under training as I am writing this. Don’t know what’ll we’ll uncover with those.

ChatGPT was thought to have no understanding. For it made basic mathematical errors. However, GPT-4 doesn’t make those mistakes. It seems on increasing the number of parameters (and number of input vectors by encoding those inputs into general language) beyond a certain threshold exponentially increases it’s overall general intelligence and understanding. And the same was concluded in this video. The result is 

Just like what happened during evolutionary growth of humans. And knowledge growth of humans has been even more exponential. The most advancements have happened in last 5 centuries only. Even more so in the last 90-100 years. Whereas we’ve been on Earth in our current evolutionary state for just a few thousand years of Earth’s entire lifetime since dawn of life(Have a look at Kurzguzagt if you are more curious). And other life forms have been here longer since that.

## AIs have 0 → 1 thinking as well

A comforting thought was given by a Stanford professor, that with advent of these generative models 0→1 thinking will play a distinguishing role in differentiating between humans and AI powered computers. I kept questioning it until I was proven right!

HOW?? Just recently I read a paper behind some LLMs capable of generating feedback on existing LLMs, helping them become even better. And avoiding the bottleneck of needing big bulky datasets. Though I need to study more about it if I can say anything more conclusively about it. Here’s the paper I read.

But, there’s more. In 2017 a new engine wa // civil eng example here

This makes the thing more exciting than before. Although it’s restrictive, the example to visualize seemingly is AlphaGo. Based off Re-inforcement learning but who’s to say that it can’t be applied in conjunction with the Generative models(who’s to say it isn’t being applied)

## It’s scary and dark because…

It’s scary because this is a proof that human brain’s are no special. Just bigger and have barely crossed a threshold giving us the intelligence advantage over other species on Earth. We are similar to these LLMs and Gollums. With the only difference being the emotional and biological regulation. Which is a chore TBH to deal and regulate. And for organisations, even more so.

Sorry to be gruesome and cruel, but we humans are LLMs based off meat and fat. The LLMs on computers are based off bits and electricity.

The big bummer is the fact that these models are all weights and biases. A big collection of floating number vectors. And hence understanding those numbers is too … big a task for anyone. No wonder they are still black box models. And just like human brain’s, understanding it’ll take time. Although that won’t happen anytime soon due to constant updates happening within these models.

## The silver lining …

These AI models are sitting idle if not in use. Except for the energy needed to keep the system running they require nothing. Meaning they don’t have a motive. And essentially are a tool to become our productivity companions. Nothing more… for now…😏

There’s a bottleneck as well. Seemingly is the lack of agility in training these LLMs. If I were to ask anything about the series of recent events, it’d fail to answer correctly. For it wasn’t trained on that data. But what if we take this basic LLM and further train it on datasets of recent events, it’d work. However that too can take weeks. Although, they are much more scalable and more trainable in comparison to humans. We take a lifetime to train/prepare. They take just a few months(years in worst case).

## In the end

It all makes me remember the science fiction books of Robot series by Issac Asimov, the Dune, Ready Player One. How things will unfold nobody can tell. One thing’s for sure, nobody’s gonna stop training larger and different LLMs. And so I’m hopping on the AI bandwagon here for my journey in Cybersecurity. Cheers.